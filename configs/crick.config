/*
 * Specific config for the Francis Crick Institute CAMP computing cluster
 */

// CAMP runs on a singularity container env
singularity {
  enabled = true
  autoMounts = true
}
docker.enabled = false

// Path to shared luslab singularity cache
// NOTE: this cannot be overriden in downstream config so it cannot be set here
//singularity.cacheDir = '/camp/lab/luscomben/home/shared/singularity'

// Crick specific computing max resource levels
params {
  // Max CPU queue parameters
  max_cpus = 32
  max_gpus = 0
  max_memory = 224.GB
  max_time = 72.h

  // Max hmem queue parameters
  max_hmem_cpus = 96
  max_hmem_gpus = 0
  max_hmem_memory = 1500.GB
  max_hmem_time = 72.h

  // Max gpu queue parameters
  max_gpu_cpus = 40
  max_gpu_gpus = 4
  max_gpu_memory = 768.GB
  max_gpu_time = 72.h
}

/*Selectors priority
When mixing generic process configuration and selectors the following priority rules are applied (from lower to higher):

1. Process generic configuration.
2. Process specific directive defined in the workflow script.
3. withLabel selector definition.
4. withName selector definition.
*/
process {
  executor = 'slurm'
  queue = 'cpu'

  // Time increases with the number of retrys
  cpus = { check_max( 1, 'cpus' ) }
  memory = { check_max( 4.GB, 'memory' ) }
  time = { check_max( 2.h * task.attempt, 'time' ) }

  errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'terminate' }
  maxRetries = 1
  maxErrors = '-1'

  // Min - mn
  // Low - l
  // Medium - m
  // High - h
  // Max - mx
  withLabel: mncpu_mnmem {
        cpus = 1
        memory = 4.GB
        queue = 'cpu'
    }

    // if(params.num_gpus == 0) {
    //   queue = 'cpu'
    // }
    // else {
    //   // Cluster options for GPU enabled instances
    //   queue = 'gpu'
    //   clusterOptions = "--gres=gpu:" + check_max( params.num_gpus, 'gpus' )

    //   // Container options for GPU enabled instances
    //   singularity.runOptions = '--nv'
    // }
}